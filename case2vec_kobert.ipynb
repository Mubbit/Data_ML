{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\5.학교 백업\\4학년 1학기\\사이버보안 졸업 프로젝트\\Recommender\\KoBERT\n"
     ]
    }
   ],
   "source": [
    "%cd KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\5.학교 백업\\\\4학년 1학기\\\\사이버보안 졸업 프로젝트\\\\Recommender\\\\KoBERT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>datanumber</th>\n",
       "      <th>title</th>\n",
       "      <th>casenumber</th>\n",
       "      <th>date</th>\n",
       "      <th>verdicttype</th>\n",
       "      <th>holding</th>\n",
       "      <th>summary</th>\n",
       "      <th>jmreference</th>\n",
       "      <th>prreference</th>\n",
       "      <th>prny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>230961</td>\n",
       "      <td>주주총회소집허가</td>\n",
       "      <td>2022마5372</td>\n",
       "      <td>20220907</td>\n",
       "      <td>결정</td>\n",
       "      <td>소수주주가 상법 제366조에 따라 임시총회 소집에 관한 법원의 허가를 신청할 때...</td>\n",
       "      <td>일반적으로 주주총회는 이사회의 결의로 소집하지만(상법 제362조)$ 예외적으로 ...</td>\n",
       "      <td>상법 제361조$ 제362조$ 제366조$ 제385조 제1항$ 제389조$ 민사소송...</td>\n",
       "      <td>대법원 2022. 4. 19. 자 2022그501 결정(공2022상$ 1013)</td>\n",
       "      <td>【신청인$ 재항고인】 신청인 (소송대리인 변호사 오원근)【사건본인$ 상대방】 주식회...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>230971</td>\n",
       "      <td>소유권이전등기</td>\n",
       "      <td>2022다217117</td>\n",
       "      <td>20220907</td>\n",
       "      <td>판결</td>\n",
       "      <td>[1] 수임인이 위임인을 위하여 자기 명의로 취득한 권리를 위임인에게 이전하여야...</td>\n",
       "      <td>[1] 민법 제684조 제2항은 \"수임인이 위임인을 위하여 자기의 명의로 취득한...</td>\n",
       "      <td>[1] 민법 제684조 / [2] 민법 제684조</td>\n",
       "      <td>[1] 대법원 2007. 2. 8. 선고 2004다64432 판결(공2007상$ ...</td>\n",
       "      <td>【원고$ 상고인 겸 피상고인】 하동지구개발사업단 주식회사의 파산관재인 (소송대리인 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>230967</td>\n",
       "      <td>폭력행위등처벌에관한법률위반(단체등의구성·활동)·폭력행위등처벌에관한법률위반(단체등의공...</td>\n",
       "      <td>2022도6993</td>\n",
       "      <td>20220907</td>\n",
       "      <td>판결</td>\n",
       "      <td>[1] 공소장변경이 허용되는 범위 / 공소사실의 동일성이 인정되지 않는 범죄사실...</td>\n",
       "      <td>[1] 공소장변경은 공소사실의 동일성이 인정되는 범위 내에서만 허용되고$ 공소사...</td>\n",
       "      <td>[1] 형사소송법 제298조 제1항 / [2] 폭력행위 등 처벌에 관한 법률 제4...</td>\n",
       "      <td>[1] 대법원 1994. 3. 22. 선고 93도2080 전원합의체 판결(공199...</td>\n",
       "      <td>【피 고 인】 피고인 1 외 1인【상 고 인】 피고인들【변 호 인】 변호사 현희철 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>230957</td>\n",
       "      <td>연차수당지급</td>\n",
       "      <td>2022다245419</td>\n",
       "      <td>20220907</td>\n",
       "      <td>판결</td>\n",
       "      <td>1년을 초과하되 2년 이하의 기간 동안 근로를 제공한 근로자에게 부여될 수 있는...</td>\n",
       "      <td>근로기준법에 따르면$ 사용자는 1년간 80% 이상 출근한 근로자에게 15일의 연...</td>\n",
       "      <td>근로기준법 제60조 제1항$ 제2항</td>\n",
       "      <td>대법원 2018. 6. 28. 선고 2016다48297 판결(공2018하$ 1435...</td>\n",
       "      <td>【원고$ 상고인】 성창산업개발 주식회사【피고$ 피상고인】 재단법인 오송첨단의료산업진...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>230981</td>\n",
       "      <td>중재판정의집행</td>\n",
       "      <td>2020마5970</td>\n",
       "      <td>20220907</td>\n",
       "      <td>결정</td>\n",
       "      <td>중재요청서 등의 서면 통지에 있어서 중재법 제4조 제3항의 발송에 의한 통지가 ...</td>\n",
       "      <td>중재절차는 당사자 간에 다른 합의가 없는 한 피신청인이 중재요청서를 받은 날부터...</td>\n",
       "      <td>중재법 제4조$ 제22조 제1항$ 제36조 제2항 제1호 (나)목$ 제38조 제1호...</td>\n",
       "      <td>-</td>\n",
       "      <td>【신청인$재항고인】 주식회사 포스코아이씨티 (소송대리인 법무법인(유한) 화우 담당변...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  datanumber                                              title  \\\n",
       "0      0      230961                                           주주총회소집허가   \n",
       "1      1      230971                                            소유권이전등기   \n",
       "2      2      230967  폭력행위등처벌에관한법률위반(단체등의구성·활동)·폭력행위등처벌에관한법률위반(단체등의공...   \n",
       "3      3      230957                                             연차수당지급   \n",
       "4      4      230981                                            중재판정의집행   \n",
       "\n",
       "    casenumber      date verdicttype  \\\n",
       "0    2022마5372  20220907          결정   \n",
       "1  2022다217117  20220907          판결   \n",
       "2    2022도6993  20220907          판결   \n",
       "3  2022다245419  20220907          판결   \n",
       "4    2020마5970  20220907          결정   \n",
       "\n",
       "                                             holding  \\\n",
       "0    소수주주가 상법 제366조에 따라 임시총회 소집에 관한 법원의 허가를 신청할 때...   \n",
       "1    [1] 수임인이 위임인을 위하여 자기 명의로 취득한 권리를 위임인에게 이전하여야...   \n",
       "2    [1] 공소장변경이 허용되는 범위 / 공소사실의 동일성이 인정되지 않는 범죄사실...   \n",
       "3    1년을 초과하되 2년 이하의 기간 동안 근로를 제공한 근로자에게 부여될 수 있는...   \n",
       "4    중재요청서 등의 서면 통지에 있어서 중재법 제4조 제3항의 발송에 의한 통지가 ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0    일반적으로 주주총회는 이사회의 결의로 소집하지만(상법 제362조)$ 예외적으로 ...   \n",
       "1    [1] 민법 제684조 제2항은 \"수임인이 위임인을 위하여 자기의 명의로 취득한...   \n",
       "2    [1] 공소장변경은 공소사실의 동일성이 인정되는 범위 내에서만 허용되고$ 공소사...   \n",
       "3    근로기준법에 따르면$ 사용자는 1년간 80% 이상 출근한 근로자에게 15일의 연...   \n",
       "4    중재절차는 당사자 간에 다른 합의가 없는 한 피신청인이 중재요청서를 받은 날부터...   \n",
       "\n",
       "                                         jmreference  \\\n",
       "0  상법 제361조$ 제362조$ 제366조$ 제385조 제1항$ 제389조$ 민사소송...   \n",
       "1                        [1] 민법 제684조 / [2] 민법 제684조   \n",
       "2   [1] 형사소송법 제298조 제1항 / [2] 폭력행위 등 처벌에 관한 법률 제4...   \n",
       "3                                근로기준법 제60조 제1항$ 제2항   \n",
       "4  중재법 제4조$ 제22조 제1항$ 제36조 제2항 제1호 (나)목$ 제38조 제1호...   \n",
       "\n",
       "                                         prreference  \\\n",
       "0       대법원 2022. 4. 19. 자 2022그501 결정(공2022상$ 1013)   \n",
       "1   [1] 대법원 2007. 2. 8. 선고 2004다64432 판결(공2007상$ ...   \n",
       "2   [1] 대법원 1994. 3. 22. 선고 93도2080 전원합의체 판결(공199...   \n",
       "3  대법원 2018. 6. 28. 선고 2016다48297 판결(공2018하$ 1435...   \n",
       "4                                                  -   \n",
       "\n",
       "                                                prny  \n",
       "0  【신청인$ 재항고인】 신청인 (소송대리인 변호사 오원근)【사건본인$ 상대방】 주식회...  \n",
       "1  【원고$ 상고인 겸 피상고인】 하동지구개발사업단 주식회사의 파산관재인 (소송대리인 ...  \n",
       "2  【피 고 인】 피고인 1 외 1인【상 고 인】 피고인들【변 호 인】 변호사 현희철 ...  \n",
       "3  【원고$ 상고인】 성창산업개발 주식회사【피고$ 피상고인】 재단법인 오송첨단의료산업진...  \n",
       "4  【신청인$재항고인】 주식회사 포스코아이씨티 (소송대리인 법무법인(유한) 화우 담당변...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case=pd.read_csv('case_list.csv')\n",
    "case.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     신청인 재항인 신청인 소송대리인 변호사 오근사건본인 상대방 주식회사 제나코리아심결정...\n",
       "1      상인 상인 하동지구개발사업단 주식회사의 파산관재인 소송대리인 무인유한 바른 담당변...\n",
       "2      1 1인 들변 호인 변호사 현희철 1인심판결 대전 2022 5 24 2021노50...\n",
       "3      상인 성창산업개발 주식회사 상인 재단인 오송첨단의료산업진흥재단 소송대리인 무인 청...\n",
       "4     신청인재항인 주식회사 포스코아이씨티 소송대리인 무인유한 화우 담당변호사 최영관 1인...\n",
       "5      상인 한솔1호발전소 주식회사 상인 소송대리인 변호사 이경익심판결 서울 2022 4...\n",
       "6      1 1인 들변 호인 변호사 윤길웅 1인심판결 부산 2022 6 22 2021노49...\n",
       "7      1 3인들 승계참가인 상인 교보자산신탁주식회사 상인 소송대리인 무인 대경종합률사무...\n",
       "8      상인 칸서스자산운용 주식회사 소송대리인 무인유한 지평 담당변호사 배기완 2인 상인...\n",
       "9      상인 상인 삼성화재해상보험 주식회사 소송대리인 무인유한 지평 담당변호사 김영수 2...\n",
       "10     상인 대한예수교장로회 교회 소송대리인 무인 다비다 담당변호사 조동섭 상인 소송대리...\n",
       "11     상인 보조참가인 상인 보조참가인 소송대리인 무인유한 광장 담당변호사 우람찬 1인심...\n",
       "12     상인 에스케이텔레콤 주식회사 소송대리인 무인유한 태평양 2인 상인 남대문세무서장 ...\n",
       "13     심판결 부산 2020 1 9 2019노472 판결주 문 상를 * 이 유 상이유를 ...\n",
       "14     상인 소송대리인 특허인 태웅 담당변리사 이태림 상인 특허청장심판결 특허 2020 ...\n",
       "15     심판결 수지 2019 5 3 2018노6585 판결주 문 심판결을 파기하 사건을 ...\n",
       "16     상인 별지 명단 기재와 * 소송대리인 무인 덕수 담당변호사 김형태 1인 상인 대한...\n",
       "17     및 검사변 호인 무인 여의 담당변호사 오영신 1인심판결 서울서부지 2020 11 ...\n",
       "18     상인 상인 소송대리인 무인 로캡 담당변호사 홍성만 상인 상인 소송대리인 변호사 신...\n",
       "19     상인 상인 한국토지주택공사 소송대리인 변호사 박인호 1인 상인 상인 김포시장 소송...\n",
       "Name: prny, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#case prny를 가공해서 kobert 만들기\n",
    "def preprocess(passage):\n",
    "    stopwords='[○\\(\\)\\[\\]\\{\\}\\$\\【\\】\\,,외,선고,법,원고탈퇴,피,]'\n",
    "    passed=re.sub(stopwords,'',passage)\n",
    "    passed=re.sub('피 고 인','피고인',passed)\n",
    "    passed=re.sub('호 소 인','호소인',passed)\n",
    "    passed=re.sub('피  인','피인',passed)\n",
    "    passed=re.sub('호 인','호인',passed)\n",
    "    passed=re.sub('상 ',' ',passed)\n",
    "    passed=re.sub('겸 ',' ',passed)\n",
    "    passed=re.sub(' 인',' ',passed)\n",
    "    passed=re.sub('[ ]+',' ',passed)\n",
    "    passed=re.sub('[가-힣]+\\.','*',passed)\n",
    "    passed=re.sub('\\.','',passed)\n",
    "    return passed\n",
    "prny=case.copy()['prny'].apply(preprocess)\n",
    "prny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_input_fy(passage):\n",
    "    return passage.split('*')\n",
    "\n",
    "bert_input=[]\n",
    "for p in prny:\n",
    "    bert_input+=bert_input_fy(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\PanryeAI\\KoBERT\n",
      " D ����̺��� ����: �� ����\n",
      " ���� �Ϸ� ��ȣ: A467-E339\n",
      "\n",
      " d:\\PanryeAI\\KoBERT ���͸�\n",
      "\n",
      "2022-12-09  ���� 04:44    <DIR>          .\n",
      "2022-12-09  ���� 04:44    <DIR>          ..\n",
      "2022-12-09  ���� 04:44    <DIR>          .git\n",
      "2022-12-09  ���� 04:44    <DIR>          .github\n",
      "2022-11-01  ���� 12:52                 8 .gitignore\n",
      "2022-12-09  ���� 04:44    <DIR>          imgs\n",
      "2022-12-09  ���� 04:44    <DIR>          kobert\n",
      "2022-12-09  ���� 04:44    <DIR>          kobert_hf\n",
      "2022-11-01  ���� 12:52            11,547 LICENSE\n",
      "2022-12-09  ���� 04:44    <DIR>          logs\n",
      "2022-11-01  ���� 12:52             9,964 README.md\n",
      "2022-11-01  ���� 12:52               200 requirements.txt\n",
      "2022-12-09  ���� 04:44    <DIR>          scripts\n",
      "2022-11-01  ���� 12:52               720 setup.py\n",
      "               5�� ����              22,439 ����Ʈ\n",
      "               9�� ���͸�  90,138,210,304 ����Ʈ ����\n"
     ]
    }
   ],
   "source": [
    "%cd KoBERT\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "#try 1\n",
    "from kobert import get_tokenizer\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Heeyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "#inputs = tokenizer.batch_encode_plus([text])\n",
    "inputs=tokenizer.batch_encode_plus(bert_input,pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking out input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs['input_ids'][2])#-len(set(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(map(len,inputs['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3030, 7119, 3969, 7846, 7119, 3030, 7119, 2839, 5808, 6122, 7119, 2345, 3417, 5546, 6495, 6383, 7119, 2664, 6305, 4230, 7957, 4128, 5655, 7535, 6745, 5417, 1660, 554, 134, 553, 538, 3886, 554, 133, 6003, 119, 141, 945, 7276, 2120, 3060, 5417, 4257, 2348, 7318, 2071, 1258, 7191, 4128, 93, 7925, 517, 7095, 6812, 2437, 7088, 4799, 5561, 7782, 3647, 2437, 2575, 1660, 5944, 6896, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4279, 2254, 7283, 7452, 6629, 7095, 535, 6416, 7095, 589, 3704, 6896, 5001, 4230, 7088, 769, 4241, 5760, 5160, 7095, 2076, 6513, 5468, 2849, 7095, 3735, 4005, 2718, 6198, 7117, 712, 7136, 6705, 7454, 6607, 7354, 7431, 5495, 6553, 498, 6003, 4965, 7088, 3703, 6896, 4156, 7815, 3831, 7454, 7095, 2849, 7088, 4484, 7836, 2872, 3854, 3552, 4484, 5330, 3854, 7086, 5176, 4297, 7436, 3283, 517, 7454, 6607, 7354, 7095, 4071, 5330, 4370, 5898, 3162, 1844, 6900, 517, 7095, 5038, 6116, 2226, 4358, 4512, 7955, 2849, 7836, 2872, 3862, 4128, 142, 198, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4636192032 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27488/2387073901.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m out = model(input_ids = torch.tensor(inputs['input_ids']),\n\u001b[0m\u001b[0;32m      2\u001b[0m               attention_mask = torch.tensor(inputs['attention_mask']))\n\u001b[0;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Heeyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Heeyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[1;32m--> 996\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Heeyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Heeyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    581\u001b[0m                 )\n\u001b[0;32m    582\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    584\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Heeyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Heeyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Heeyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Heeyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     ):\n\u001b[1;32m--> 400\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Heeyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Heeyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    320\u001b[0m                 \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrelative_position_scores_query\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrelative_position_scores_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4636192032 bytes."
     ]
    }
   ],
   "source": [
    "out = model(input_ids = torch.tensor(inputs['input_ids']),\n",
    "              attention_mask = torch.tensor(inputs['attention_mask']))\n",
    "out.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27488/1163473658.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../dataset/case_nm_tensor.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(out.pooler_output, '../dataset/case_nm_tensor.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b485adf93bff8ca269d5493edbedeaa1a759650bfde9cbea36fd7f7b559c92d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
